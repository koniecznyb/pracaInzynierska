\chapter{Implementacja}
\label{cha:implementacja}

Agent mając wiedzę na temat
\begin{itemize*}
\renewcommand{\labelitemi}{$\bullet$}
 \item akcji, które może wykonać,
 \item stanu, w jakim się znajduje,
 \item zasad, definiujących warunki otrzymania nagrody,
 \item polityki wybierania akcji
\end{itemize*}
jest w stanie dostosować swoje kolejne działania (wybrane akcje), tak aby uzyskać jak
najlepszy wynik w kolejnych iteracjach symulacji. Celem agenta jest dotarcie do punktu końcowego, zdobywając jak 
największą nagrodę. \\
\indent Po osiągnięciu celu symulacja jest resetowana, jej wynik jest zapisywany. Robot ucząc się na podstawie 
poprzednio dokonanych wyborów, buduje tablicę mapującą stan-akcja $Q(S, A)$ do konkretnej wyliczonej wartości. W 
programie do przedstawienia wartości tablicy stan-akcja $Q(S, A)$ wykorzystano wielowymiarową tablicę.

\section{Symulacja graficzna}
\label{sec:symulacjagraficzna}

Jako symulacja graficzna wykorzystano figury geometryczne, reprezentujące agenta, przeszkody, granice i cel.
Agent porusza się na dwuwymiarowej przestrzeni o wymiarach $50x50$ pikseli.

\section{Reprezentacja stanu}



\subsection{Wybór algorytmów}
\label{subsec:wyboralgorytmow}

\subsubsection{Q-learning}
\label{subsubsec:qlearning}

\subsubsection{SARSA}
\label{subsubsec:sarsa}

\section{Wykorzystane technologie}
\label{sec:wykorzystanetechnologie}

Java 8 (środowisko IntelliJ Idea)
LibGdx (symulacja graficzna)
Project Lombok
Apache POI(czytanie i zapisywanie do xml)


\section{Napotkane problemy}
\label{sec:napotkaneproblemy}

Reprezentacja stanu
Testowanie działania algorytmów

\section{Wynik działania}
\label{sec:wynikdzialania}


%---------------------------------------------------------------------------















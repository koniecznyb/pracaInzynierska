\chapter{Podsumowanie}
\label{cha:podsumowanie}

Zaimplementowany agent poprawnie nauczył się osiągać cel w zaprojektowanym środowisku. Odpowiednio zaimplementowany 
algorytm Q-learning pozwolił robotowi na efektywne generowanie wyników tablicy $Q(S, A)$, dzięki czemu bardzo szybko 
odnalazł optymalną drogę do osiągnięcia stanu końcowego. Ze względu na prostotę środowiska agent 
zaledwie po około siódmej iteracji osiąga najkorzystniejszy wynik. Dla lepszych testów działania algorytmu w 
przyszłości odpowiednie byłoby wprowadzenie bardziej zaawansowanej struktury środowiska. Analogicznie istotną kwestią 
byłby poprawny dobór parametrów algorytmu Q-learning oraz usprawniona reprezentacja stanu.
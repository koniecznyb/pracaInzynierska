\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\select@language {polish}
\contentsline {chapter}{\numberline {1}Wst\IeC {\k e}p}{7}
\contentsline {section}{\numberline {1.1}Cele pracy}{7}
\contentsline {section}{\numberline {1.2}Zawarto\IeC {\'s}\IeC {\'c} pracy}{8}
\contentsline {chapter}{\numberline {2}Uczenie maszynowe - rozdzia\IeC {\l } teoretyczny}{9}
\contentsline {section}{\numberline {2.1}Podej\IeC {\'s}cia do uczenia maszynowego}{9}
\contentsline {subsection}{\numberline {2.1.1}Uczenie nadzorowane}{9}
\contentsline {subsection}{\numberline {2.1.2}Uczenie nienadzorowane}{10}
\contentsline {subsection}{\numberline {2.1.3}Uczenie ze wzmocnieniem}{12}
\contentsline {subsubsection}{\numberline {2.1.3.1}Q-learning}{14}
\contentsline {section}{\numberline {2.2}Podsumowanie}{14}
\contentsline {chapter}{\numberline {3}Implementacja}{15}
\contentsline {section}{\numberline {3.1}Struktura stan-akcje $Q(S, A)$}{15}
\contentsline {section}{\numberline {3.2}Polityka wyboru akcji}{16}
\contentsline {subsection}{\numberline {3.2.1}R\IeC {\'o}wnowaga pomi\IeC {\k e}dzy eksploracj\IeC {\k a}, a eksploatacj\IeC {\k a}}{16}
\contentsline {subsection}{\numberline {3.2.2}$\epsilon $-greedy policy}{17}
\contentsline {subsection}{\numberline {3.2.3}Optimal policy}{17}
\contentsline {section}{\numberline {3.3}Symulacja graficzna}{17}
\contentsline {section}{\numberline {3.4}Reprezentacja stanu}{18}
\contentsline {subsection}{\numberline {3.4.1}Akcje}{20}
\contentsline {subsection}{\numberline {3.4.2}Warto\IeC {\'s}\IeC {\'c} nagr\IeC {\'o}d}{21}
\contentsline {subsection}{\numberline {3.4.3}Wyb\IeC {\'o}r algorytm\IeC {\'o}w}{21}
\contentsline {section}{\numberline {3.5}Wykorzystane technologie}{24}
\contentsline {section}{\numberline {3.6}Napotkane problemy}{24}
\contentsline {section}{\numberline {3.7}Obs\IeC {\l }uga symulacji}{24}
\contentsline {section}{\numberline {3.8}Wynik dzia\IeC {\l }ania}{25}
\contentsline {chapter}{\numberline {4}Podsumowanie}{27}

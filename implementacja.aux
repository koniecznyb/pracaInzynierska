\relax 
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Implementacja}{15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{cha:implementacja}{{3}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Struktura stan-akcje $Q(S, A)$}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Przyk\IeC {\l }adowy wynik dzia\IeC {\l }ania algorytmu Q-learning. Mo\IeC {\.z}na zauwa\IeC {\.z}y\IeC {\'c}, \IeC {\.z}e do ka\IeC {\.z}dego stanu przydzielone s\IeC {\k a} akcje wraz z warto\IeC {\'s}ci\IeC {\k a} skumulowanej nagrody wyliczonej za pomoc\IeC {\k a} algorytmu.\relax }}{16}}
\newlabel{fig:przykladowywpis}{{3.1}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Liczba w samym \IeC {\'s}rodku reprezentuje numer stanu. Bezpo\IeC {\'s}redni\IeC {\k a} z ni\IeC {\k a} s\IeC {\k a}siaduj\IeC {\k a}ce s\IeC {\k a} typy obiekt\IeC {\'o}w, \textbf  {B}order - granica, \textbf  {O}bstacle - przeszkoda, \textbf  {E}mpty - puste pole, \textbf  {P}rize - nagroda (stan ko\IeC {\'n}cowy)\relax }}{16}}
\newlabel{fig:wyjasnieniewpisu}{{3.2}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Polityka wyboru akcji}{16}}
\newlabel{sec:politykawyboru}{{3.2}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}R\IeC {\'o}wnowaga pomi\IeC {\k e}dzy eksploracj\IeC {\k a}, a eksploatacj\IeC {\k a}}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}$\epsilon $-greedy policy}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.3}Optimal policy}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Symulacja graficzna}{17}}
\newlabel{sec:symulacjagraficzna}{{3.3}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Przyk\IeC {\l }adowy stan symulacji\relax }}{17}}
\newlabel{fig:symulacja}{{3.3}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Przyk\IeC {\l }adowy stan symulacji\relax }}{18}}
\newlabel{fig:symulacja2}{{3.4}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Reprezentacja stanu}{18}}
\newlabel{sec:reprezentacjastanu}{{3.4}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Agent graniczy od g\IeC {\'o}rnej strony z przeszkod\IeC {\k a}, natomiast w pozosta\IeC {\l }ych kierunkach znajduj\IeC {\k a} si\IeC {\k e} puste pola\relax }}{19}}
\newlabel{fig:grid1}{{3.5}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Agent s\IeC {\k a}siaduje od g\IeC {\'o}ry i lewej strony z granic\IeC {\k a}, oznacza to, \IeC {\.z}e robot znajduje si\IeC {\k e} w g\IeC {\'o}rnym lewym rogu mapy, a jedynymi akcjami jakie mo\IeC {\.z}e wykona\IeC {\'c} to ruch w prawo i ruch w d\IeC {\'o}\IeC {\l }\relax }}{19}}
\newlabel{fig:grid2}{{3.6}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Agent s\IeC {\k a}siaduje od g\IeC {\'o}ry i lewej strony z granic\IeC {\k a}, a od prawej z stanem ko\IeC {\'n}cowym. Agent wykonuj\IeC {\k a}c ruch w prawo osi\IeC {\k a}gnie sw\IeC {\'o}j cel, powoduj\IeC {\k a}c tym samym zako\IeC {\'n}czenie i zresetowanie symulacji\relax }}{20}}
\newlabel{fig:grid3}{{3.7}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Agent graniczy z ka\IeC {\.z}dej strony z przeszkod\IeC {\k a}, jakikolwiek ruch powoduje utrat\IeC {\k e} punkt\IeC {\'o}w\relax }}{20}}
\newlabel{fig:grid4}{{3.8}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Akcje}{20}}
\newlabel{subsec:akcje}{{3.4.1}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Warto\IeC {\'s}\IeC {\'c} nagr\IeC {\'o}d}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Wyb\IeC {\'o}r algorytm\IeC {\'o}w}{21}}
\newlabel{subsec:wyboralgorytmow}{{3.4.3}{21}}
\newlabel{lst:qlearning}{{3.1}{21}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.1}Implementacja algorytmu Q-learning}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Wykorzystane technologie}{24}}
\newlabel{sec:wykorzystanetechnologie}{{3.5}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Napotkane problemy}{24}}
\newlabel{sec:napotkaneproblemy}{{3.6}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Obs\IeC {\l }uga symulacji}{24}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Wynik dzia\IeC {\l }ania}{25}}
\newlabel{sec:wynikdzialania}{{3.8}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Przyk\IeC {\l }adowy wynik dzia\IeC {\l }ania algorytmu. Mo\IeC {\.z}na zaobserwowa\IeC {\'c} poprawnie wyznaczone warto\IeC {\'s}ci akcji. Nagroda znajduje si\IeC {\k e} w lewym g\IeC {\'o}rnym rogu mapy, dlatego te\IeC {\.z} nie op\IeC {\l }aca si\IeC {\k e} agentowi wykona\IeC {\'c} akcj\IeC {\k e} w przeciwnym kierunku. Agent je\IeC {\.z}eli ma tak\IeC {\k a} mo\IeC {\.z}liwo\IeC {\'s}\IeC {\'c} wykonuje ruch w takich stan, \IeC {\.z}eby nie znale\IeC {\'z}\IeC {\'c} si\IeC {\k e} na przeszkodzie\relax }}{25}}
\newlabel{fig:wynik1}{{3.9}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Przyk\IeC {\l }adowy wynik dzia\IeC {\l }ania algorytmu. Analogicznie jak na obrazku wy\IeC {\.z}ej, agent wykonuje podobne wybory. Mo\IeC {\.z}na zauwa\IeC {\.z}y\IeC {\'c}, \IeC {\.z}e robot znajduj\IeC {\k a}c si\IeC {\k e} wewn\IeC {\k a}trz przeszkody (stan nr. 15), ka\IeC {\.z}dy ruch uwa\IeC {\.z}a za niekorzystny ale jednak z delikatn\IeC {\k a} przewag\IeC {\k a} ruchu do g\IeC {\'o}ry lub w lewo (w kierunku nagrody). W przypadku, gdy agent graniczy z lewej strony z przeszkod\IeC {\k a} jak w stanie nr. 16, niedost\IeC {\k e}pna jest akcja MOVE LEFT\relax }}{26}}
\newlabel{fig:wynik2}{{3.10}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Przyk\IeC {\l }adowy wynik dzia\IeC {\l }ania algorytmu. Na tym obrazku wida\IeC {\'c} stan nr. 68, w kt\IeC {\'o}rym agent nigdy si\IeC {\k e} nie znalaz\IeC {\l }. Jest to logiczna konsekwencja tego, \IeC {\.z}e nagroda (stan ko\IeC {\'n}cowy) znajduje si\IeC {\k e} w lewym g\IeC {\'o}rnym obszarze mapy, a wi\IeC {\k e}c nie mo\IeC {\.z}e s\IeC {\k a}siadowa\IeC {\'c} z granic\IeC {\k a} od do\IeC {\l }u\relax }}{26}}
\newlabel{fig:wynik3}{{3.11}{26}}
\@setckpt{implementacja}{
\setcounter{page}{27}
\setcounter{equation}{0}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{8}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{11}
\setcounter{table}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{parentequation}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{17}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextrayear}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{lstnumber}{75}
\setcounter{nlinenum}{0}
\setcounter{float@type}{8}
\setcounter{lstlisting}{1}
}
